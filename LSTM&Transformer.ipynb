{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu1Plf+KZ5unE+QIDNJhjC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ScientistLim/ProjectAW/blob/feature%2Fmodel%2Flstm-transformer/LSTM%26Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh2nW8MjkXNv",
        "outputId": "04d186f3-2bc3-4e3a-b766-8d9a302356b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-forecasting\n",
        "!pip instsall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8XvHsgUuYNE",
        "outputId": "e990ab8b-0506-4a6e-9395-aa0680ad00c1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-forecasting\n",
            "  Downloading pytorch_forecasting-1.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.26.4)\n",
            "Requirement already satisfied: torch!=2.0.1,<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (2.4.1+cu121)\n",
            "Collecting lightning<3.0.0,>=2.0.0 (from pytorch-forecasting)\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.13.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.5.2)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2024.6.1)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (24.1)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n",
            "  Downloading torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.10.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (71.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.2.0)\n",
            "Downloading pytorch_forecasting-1.1.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.4.3-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning, pytorch-forecasting\n",
            "Successfully installed lightning-2.4.0 lightning-utilities-0.11.7 pytorch-forecasting-1.1.1 pytorch-lightning-2.4.0 torchmetrics-1.4.3\n",
            "ERROR: unknown command \"instsall\" - maybe you meant \"install\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer with LSTM"
      ],
      "metadata": {
        "id": "GScCKa5goI31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Bidirectional, LSTM\n",
        "from keras.initializers import Ones, Zeros\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Lambda\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "j0D2UEtnoU7i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 엑셀 파일 불러오기\n",
        "file_path = '/content/drive/MyDrive/종설프1_5팀/전처리.ipynb/four-cycle-16months.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# 데이터 전처리: 필요한 컬럼 설정 (Time, feed_pressure)\n",
        "df['time'] = pd.to_datetime(df['time'])\n",
        "df['time_idx'] = ((df['time'] - df['time'].min()).dt.total_seconds() // 300).astype(int)  # 5분 간격의 시간 인덱스 생성\n",
        "df['group'] = \"feed_pressure\"  # 그룹화 컬럼 추가\n",
        "\n",
        "# 데이터 확인\n",
        "print(df.head())\n",
        "\n",
        "# 데이터 길이 확인\n",
        "total_length = len(df)\n",
        "print(f\"Total data length: {total_length} rows\")"
      ],
      "metadata": {
        "id": "ZVRnFzMfHflW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfea6e2a-eef8-40a5-813f-ec6980423387"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 time  feed_pressure  time_idx          group\n",
            "0 2024-04-07 23:33:31       0.000000     97161  feed_pressure\n",
            "1 2024-04-07 23:34:31       0.003304     97161  feed_pressure\n",
            "2 2024-04-07 23:35:31       0.006620     97162  feed_pressure\n",
            "3 2024-04-07 23:36:31       0.009646     97162  feed_pressure\n",
            "4 2024-04-07 23:37:31       0.014007     97162  feed_pressure\n",
            "Total data length: 971616 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이상치 탐지 함수\n",
        "def get_outlier(df=None, column=None, weight=1.5):\n",
        "    quantile_25 = np.percentile(df[column].values, 25)\n",
        "    quantile_75 = np.percentile(df[column].values, 75)\n",
        "    IQR = quantile_75 - quantile_25\n",
        "    IQR_weight = IQR * weight\n",
        "    lowest = quantile_25 - IQR_weight * 1.423 / 1.5\n",
        "    highest = quantile_75 + IQR_weight * 1.6 / 1.5\n",
        "    outlier_idx = df[column][(df[column] < lowest) | (df[column] > highest)].index\n",
        "    return outlier_idx\n",
        "\n",
        "# 이상치 제거\n",
        "outlier_idx = get_outlier(df=df, column='feed_pressure', weight=1.5)\n",
        "df.drop(outlier_idx, axis=0, inplace=True)\n",
        "\n",
        "# 데이터 시퀀스 생성 함수\n",
        "def create_sequences(data, sequence_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i:i + sequence_length])\n",
        "        y.append(data[i + sequence_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 시퀀스 길이\n",
        "sequence_length = 500\n",
        "\n",
        "# Feed pressure 데이터 시퀀스 생성\n",
        "X_feed = df[\"feed_pressure\"].values\n",
        "X_feed_seq, y_feed_seq = create_sequences(X_feed, sequence_length)\n",
        "\n",
        "# 데이터 분할\n",
        "X_feed_train, X_feed_test, y_feed_train, y_feed_test = train_test_split(X_feed_seq, y_feed_seq, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "my5HmGKXcFku"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 함수 정의"
      ],
      "metadata": {
        "id": "0Z_VJrf9tTe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MultiHeadAttention 클래스 정의\n",
        "class LayerNormalization(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        mean = tf.keras.backend.mean(inputs, axis=-1, keepdims=True)\n",
        "        std = tf.keras.backend.std(inputs, axis=-1, keepdims=True)\n",
        "        return self.gamma * (inputs - mean) / (std + self.eps) + self.beta\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(LayerNormalization, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:], initializer=Ones(), trainable=True)\n",
        "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer=Zeros(), trainable=True)\n",
        "        super(LayerNormalization, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        mean = tf.keras.backend.mean(x, axis=-1, keepdims=True)\n",
        "        std = tf.keras.backend.std(x, axis=-1, keepdims=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "L5k4an9YdSwv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, attn_dropout=0.1):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.temper = np.sqrt(d_model)\n",
        "        self.dropout = Dropout(attn_dropout)\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        attn = tf.keras.backend.batch_dot(q, k, axes=[2, 2]) / self.temper\n",
        "        if mask is not None:\n",
        "            attn += (-1e+10) * (1 - mask)\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = tf.keras.backend.batch_dot(attn, v)\n",
        "        return output, attn\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.dropout = dropout\n",
        "        self.qs_layer = Dense(n_head * d_k, use_bias=False)\n",
        "        self.ks_layer = Dense(n_head * d_k, use_bias=False)\n",
        "        self.vs_layer = Dense(n_head * d_v, use_bias=False)\n",
        "        self.attention = ScaledDotProductAttention(d_model, attn_dropout=dropout)\n",
        "        self.w_o = Dense(d_model)\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Linear projections\n",
        "        qs = self.qs_layer(q)  # [batch_size, len_q, n_head * d_k]\n",
        "        ks = self.ks_layer(k)  # [batch_size, len_k, n_head * d_k]\n",
        "        vs = self.vs_layer(v)  # [batch_size, len_v, n_head * d_v]\n",
        "\n",
        "        # Split heads\n",
        "        qs = tf.reshape(qs, (batch_size, -1, self.n_head, self.d_k))\n",
        "        ks = tf.reshape(ks, (batch_size, -1, self.n_head, self.d_k))\n",
        "        vs = tf.reshape(vs, (batch_size, -1, self.n_head, self.d_v))\n",
        "\n",
        "        # Transpose for attention dot product: [batch_size, n_head, len_q, d_k]\n",
        "        qs = tf.transpose(qs, [0, 2, 1, 3])\n",
        "        ks = tf.transpose(ks, [0, 2, 1, 3])\n",
        "        vs = tf.transpose(vs, [0, 2, 1, 3])\n",
        "\n",
        "        # Reshape for batch computation\n",
        "        qs = tf.reshape(qs, (-1, tf.shape(qs)[2], self.d_k))  # [batch_size * n_head, len_q, d_k]\n",
        "        ks = tf.reshape(ks, (-1, tf.shape(ks)[2], self.d_k))  # [batch_size * n_head, len_k, d_k]\n",
        "        vs = tf.reshape(vs, (-1, tf.shape(vs)[2], self.d_v))  # [batch_size * n_head, len_v, d_v]\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = tf.keras.backend.repeat_elements(mask, self.n_head, axis=0)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        head, attn = self.attention(qs, ks, vs, mask)\n",
        "\n",
        "        # Reshape back to original\n",
        "        head = tf.reshape(head, (batch_size, self.n_head, -1, self.d_v))\n",
        "        head = tf.transpose(head, [0, 2, 1, 3])\n",
        "        head = tf.reshape(head, (batch_size, -1, self.n_head * self.d_v))\n",
        "\n",
        "        # Final linear layer\n",
        "        outputs = self.w_o(head)\n",
        "        outputs = Dropout(self.dropout)(outputs)\n",
        "        return self.layer_norm(outputs), attn\n"
      ],
      "metadata": {
        "id": "UrljaWgyyI4H"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 정의"
      ],
      "metadata": {
        "id": "tLxJhGBls25K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialLSTMTransformer():\n",
        "    def __init__(self, maxlen, max_features, embed_size, n_head, d_k, d_v):\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embed_size = embed_size\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "    def build_model(self):\n",
        "        src_seq_input = Input(shape=(self.maxlen,))\n",
        "        x = Embedding(input_dim=self.max_features, output_dim=self.embed_size)(src_seq_input)\n",
        "\n",
        "        # LSTM Layers\n",
        "        lstm_out = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "        lstm_out = Bidirectional(LSTM(64, return_sequences=True))(lstm_out)\n",
        "\n",
        "        # Transformer Layers\n",
        "        transformer_out, _ = MultiHeadAttention(n_head=self.n_head, d_model=self.embed_size, d_k=self.d_k, d_v=self.d_v, dropout=0.1)(lstm_out, lstm_out, lstm_out)\n",
        "\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling1D()(transformer_out)\n",
        "        max_pool = GlobalMaxPooling1D()(transformer_out)\n",
        "        conc = concatenate([avg_pool, max_pool])\n",
        "\n",
        "        # Fully connected layer\n",
        "        conc = Dense(64, activation=\"relu\")(conc)\n",
        "        output = Dense(1, activation=\"sigmoid\")(conc)\n",
        "\n",
        "        self.model = Model(inputs=src_seq_input, outputs=output)\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return self.model"
      ],
      "metadata": {
        "id": "HaueCuyBymoe"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM-Transformer 병렬 모델 정의\n",
        "class ParallelLSTMTransformer():\n",
        "    def __init__(self, maxlen, max_features, embed_size, n_head, d_k, d_v):\n",
        "        self.maxlen = maxlen\n",
        "        self.max_features = max_features\n",
        "        self.embed_size = embed_size\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "    def build_model(self):\n",
        "        src_seq_input = Input(shape=(self.maxlen,))\n",
        "        x = Embedding(input_dim=self.max_features, output_dim=self.embed_size)(src_seq_input)\n",
        "\n",
        "        # LSTM Branch\n",
        "        lstm_branch = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "        lstm_branch = Bidirectional(LSTM(64, return_sequences=True))(lstm_branch)\n",
        "\n",
        "        # Transformer Branch\n",
        "        transformer_branch, _ = MultiHeadAttention(n_head=self.n_head, d_model=self.embed_size, d_k=self.d_k, d_v=self.d_v, dropout=0.1)(x, x, x)\n",
        "\n",
        "        # Concatenate LSTM and Transformer outputs\n",
        "        merged = concatenate([lstm_branch, transformer_branch])\n",
        "\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling1D()(merged)\n",
        "        max_pool = GlobalMaxPooling1D()(merged)\n",
        "        conc = concatenate([avg_pool, max_pool])\n",
        "\n",
        "        # Fully connected layer\n",
        "        conc = Dense(64, activation=\"relu\")(conc)\n",
        "        output = Dense(1, activation=\"sigmoid\")(conc)\n",
        "\n",
        "        self.model = Model(inputs=src_seq_input, outputs=output)\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return self.model"
      ],
      "metadata": {
        "id": "cf2dYwXhsu7W"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 생성 및 시각화"
      ],
      "metadata": {
        "id": "KP7IC1FTszaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 직렬 LSTM-Transformer 모델 생성 및 TensorBoard 사용 설정\n",
        "sequential_model = SequentialLSTMTransformer(maxlen=500, max_features=20000, embed_size=300, n_head=3, d_k=64, d_v=64)\n",
        "model = sequential_model.build_model()\n",
        "\n",
        "# TensorBoard 설정\n",
        "\n",
        "example_input = torch.randn(1, 500)  # 입력 크기를 모델에 맞게 조정\n",
        "# 모델의 구조를 요약하여 출력\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "rc313aHmyseu",
        "outputId": "b8e78181-2a04-4010-f7c6-e86dddba8a41"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_13            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │      \u001b[38;5;34m6,000,000\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m439,296\u001b[0m │ embedding_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m164,352\u001b[0m │ bidirectional_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_4    │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m),    │        \u001b[38;5;34m132,228\u001b[0m │ bidirectional_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)]    │                │ bidirectional_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ bidirectional_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_58 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m38,464\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_59 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dense_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_13            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,000</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">439,296</span> │ embedding_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ bidirectional_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_4    │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>),    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">132,228</span> │ bidirectional_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)]    │                │ bidirectional_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ bidirectional_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">38,464</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,774,405\u001b[0m (25.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,774,405</span> (25.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,774,405\u001b[0m (25.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,774,405</span> (25.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 병렬 LSTM-Transformer 모델 생성 및 TensorBoard 사용 설정\n",
        "parallel_model = ParallelLSTMTransformer(maxlen=500, max_features=20000, embed_size=300, n_head=3, d_k=64, d_v=64)\n",
        "model = parallel_model.build_model()\n",
        "\n",
        "# TensorBoard 설정\n",
        "\n",
        "example_input = torch.randn(1, 500)  # 입력 크기를 모델에 맞게 조정\n",
        "# 모델의 구조를 요약하여 출력\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "4O9D8OAEsH-0",
        "outputId": "0889b608-5df9-4149-c51d-efdc2372d619"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_14            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_14 (\u001b[38;5;33mEmbedding\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │      \u001b[38;5;34m6,000,000\u001b[0m │ input_layer_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_28          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m439,296\u001b[0m │ embedding_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_29          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m164,352\u001b[0m │ bidirectional_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_5    │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m),    │        \u001b[38;5;34m231,300\u001b[0m │ embedding_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)]    │                │ embedding_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                           │                        │                │ embedding_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m428\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ bidirectional_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ multi_head_attention_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m428\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_2    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m428\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m856\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_64 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m54,848\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_65 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dense_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_14            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,000,000</span> │ input_layer_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_28          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">439,296</span> │ embedding_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional_29          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ bidirectional_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_5    │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>),    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">231,300</span> │ embedding_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)]    │                │ embedding_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                           │                        │                │ embedding_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">428</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ multi_head_attention_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">428</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_2    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">428</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">856</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">54,848</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,889,861\u001b[0m (26.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,889,861</span> (26.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,889,861\u001b[0m (26.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,889,861</span> (26.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Lambda  # Lambda 함수 임포트\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Transformer 모델 생성 및 정의 (위 코드 동일)\n",
        "transformer = Transformer(maxlen=100, max_features=20000, embed_size=300, n_head=3, d_k=64, d_v=64)\n",
        "model = transformer.build_model()\n",
        "\n",
        "# 모델 구조 시각화\n",
        "plot_model(model, to_file='transformer_model.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# 이미지 표시\n",
        "img = plt.imread('transformer_model.png')\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YactvWIen2U7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "37273619-9bc9-48a5-fbae-a4c12a95fc29"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Transformer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-03adf639e873>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Transformer 모델 생성 및 정의 (위 코드 동일)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Transformer' is not defined"
          ]
        }
      ]
    }
  ]
}